{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec320610",
   "metadata": {},
   "source": [
    "# <center>This `.ipynb` file contains the code for training the autoencoder</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4bf4f5",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2eb0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pfiles.lpips import LPIPS\n",
    "from pfiles.vqvae import VQVAE\n",
    "from pfiles.discriminator import Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99957f1",
   "metadata": {},
   "source": [
    "### 2. Define the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ad056",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device is:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524f6ae",
   "metadata": {},
   "source": [
    "### 3. Set different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 765\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bab65b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_batch_size = 16\n",
    "rgb_input = 3\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39170dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_step_start = 1000\n",
    "step_count = 0\n",
    "acc_steps = 1\n",
    "codebook_weight = 1\n",
    "commitment_beta = 0.2\n",
    "disc_weight = 0.5\n",
    "perceptual_weight = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c9580",
   "metadata": {},
   "source": [
    "### 4. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd1eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_src = '/project/dsc-is/nono/Documents/kpc/dat0'\n",
    "data_src = 'slice128_Block2_11K.npy'\n",
    "\n",
    "print(os.path.join(dir_src, data_src))\n",
    "\n",
    "kpc_dataset = np.load(os.path.join(dir_src, data_src))\n",
    "kpc_dataset = kpc_dataset[:, 0, :, :, :]\n",
    "\n",
    "print(kpc_dataset.shape)\n",
    "N_SAMPLE, HEIGHT, WIDTH, CHANNELS = kpc_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09bf4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_range = np.arange(N_SAMPLE)\n",
    "split = np.array_split(index_range, 11)\n",
    "test_dataset = split[10]\n",
    "training_dataset = np.setdiff1d(index_range, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72347f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of the training dataset:', len(training_dataset))\n",
    "print('Length of the test dataset:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903c2b6",
   "metadata": {},
   "source": [
    "### 5. Custom functions for extracting batches of samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47cd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_list(idx, n_batch=10, batch_size=None, shuffle=True):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    if batch_size is not None:\n",
    "        n_batch = len(idx) // batch_size\n",
    "    batch_list = np.array_split(idx, n_batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bd55af",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "def generate_batch(idx, kpc_dataset):\n",
    "    tmp = []\n",
    "    for i in idx:\n",
    "        xxx = transform(kpc_dataset[i])\n",
    "        tmp.append(xxx)\n",
    "    xxx_batch = torch.stack(tmp, dim=0)\n",
    "    return xxx_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163acdb6",
   "metadata": {},
   "source": [
    "### 6. Set up directory for saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'kpc_ldm'\n",
    "\n",
    "if not os.path.exists(task_name):\n",
    "    os.mkdir(task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0c53a",
   "metadata": {},
   "source": [
    "### 7. Instantiate `VQVAE`, `LPIPS model`, and `Discriminator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1632627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = VQVAE(im_channels=3).to(device)\n",
    "model.train()\n",
    "lpips_model = LPIPS().eval().to(device)\n",
    "discrim = Discriminator(im_channels=3).to(device)\n",
    "discrim.train()\n",
    "\n",
    "# setting up additional hyperparameters\n",
    "recon_criterion = nn.MSELoss()\n",
    "disc_criterion = nn.MSELoss()\n",
    "\n",
    "optimizer_g = optim.Adam(model.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discrim.parameters(), lr=0.0001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe53e7e",
   "metadata": {},
   "source": [
    "### 8. Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fdae98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_idx in range(num_epochs):\n",
    "    batch_list = make_batch_list(training_dataset, batch_size=select_batch_size)\n",
    "    \n",
    "    recon_losses = []\n",
    "    codebook_losses = []\n",
    "    \n",
    "    perceptual_losses = []\n",
    "    \n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "    losses = []\n",
    "    \n",
    "    optimizer_d.zero_grad()\n",
    "    optimizer_g.zero_grad()\n",
    "    \n",
    "    for idx_tmp in tqdm(batch_list):\n",
    "        \n",
    "        step_count += 1\n",
    "        xxx_tmp = generate_batch(idx_tmp, kpc_dataset)\n",
    "        im = xxx_tmp.to(device)\n",
    "        \n",
    "        model_output = model(im)\n",
    "        output, z, quantize_losses = model_output\n",
    "            \n",
    "        recon_loss = recon_criterion(output, im)\n",
    "        recon_losses.append(recon_loss.item())\n",
    "        recon_loss = recon_loss / acc_steps\n",
    "        \n",
    "        g_loss = (recon_loss + (codebook_weight * quantize_losses['codebook_loss'] / acc_steps) +\\\n",
    "                               (commitment_beta * quantize_losses['commitment_loss'] / acc_steps))\n",
    "        \n",
    "        codebook_losses.append(codebook_weight * quantize_losses['codebook_loss'].item())\n",
    "        \n",
    "        if step_count > disc_step_start:\n",
    "            disc_fake_pred = discrim(model_output[0])\n",
    "            disc_fake_loss = disc_criterion(disc_fake_pred, torch.ones(disc_fake_pred.shape, device=disc_fake_pred.device))\n",
    "            \n",
    "            gen_losses.append(disc_weight * disc_fake_loss.item())\n",
    "            g_loss += disc_weight * disc_fake_loss / acc_steps\n",
    "            \n",
    "        lpips_loss = torch.mean(lpips_model(output, im)) / acc_steps\n",
    "        perceptual_losses.append(perceptual_weight * lpips_loss.item())\n",
    "        g_loss += perceptual_weight * lpips_loss / acc_steps\n",
    "        \n",
    "        losses.append(g_loss.item())\n",
    "        g_loss.backward()\n",
    "        \n",
    "        if step_count > disc_step_start:\n",
    "            \n",
    "            fake = output\n",
    "            \n",
    "            disc_fake_pred = discrim(fake.detach())\n",
    "            disc_real_pred = discrim(im)\n",
    "            \n",
    "            disc_fake_loss = disc_criterion(disc_fake_pred, torch.zeros(disc_fake_pred.shape, device=disc_fake_pred.device))\n",
    "            disc_real_loss = disc_criterion(disc_real_pred, torch.ones(disc_real_pred.shape, device=disc_real_pred.device))\n",
    "            \n",
    "            disc_loss = disc_weight * (disc_fake_loss + disc_real_loss) / 2\n",
    "            disc_losses.append(disc_loss.item())\n",
    "            \n",
    "            disc_loss = disc_loss / acc_steps\n",
    "            disc_loss.backward()\n",
    "            \n",
    "            if step_count % acc_steps == 0:\n",
    "                optimizer_d.step()\n",
    "                optimizer_d.zero_grad()\n",
    "                \n",
    "        if step_count % acc_steps == 0:\n",
    "            optimizer_g.step()\n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "    optimizer_d.step()\n",
    "    optimizer_d.zero_grad()\n",
    "    optimizer_g.step()\n",
    "    optimizer_g.zero_grad()\n",
    "    \n",
    "    if len(disc_losses) > 0:\n",
    "        print('Finished epoch: {} | Recon loss: {:.4f} | Perceptual loss: {:.4f} | Codebook: {:.4f} | G loss: {:.4f} | '\n",
    "              'D loss: {:.4f}'.format(epoch_idx + 1, np.mean(recon_losses), np.mean(perceptual_losses),\n",
    "                                                     np.mean(codebook_losses), np.mean(gen_losses), np.mean(disc_losses)))\n",
    "        \n",
    "    else:\n",
    "        print('Finished epoch: {} | Recon loss: {:.4f} | Perceptual loss: {:.4f} | Codebook: {:.4f}'.format(epoch_idx + 1,\n",
    "                                                     np.mean(recon_losses), np.mean(perceptual_losses),\n",
    "                                                     np.mean(codebook_losses)))\n",
    "\n",
    "print('Done training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144b217",
   "metadata": {},
   "source": [
    "### 9. Save models after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553a3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(task_name, 'vqvae_autoencoder_ckpt.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
