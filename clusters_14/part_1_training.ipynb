{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4087d534",
   "metadata": {},
   "source": [
    "# <center>This `.ipynb` file contains the code for training the `cLDM` architecture</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f41fea",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65377e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from pfiles.unet_cond_base import UNet\n",
    "from pfiles.vqvae import VQVAE\n",
    "from pfiles.linear_noise_scheduler import LinearNoiseScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db4362",
   "metadata": {},
   "source": [
    "### 2. Define a stamp to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    time_cur = datetime.datetime.now()\n",
    "    stamp = time_cur.strftime('%Y%m%d')\n",
    "    return stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b878155",
   "metadata": {},
   "outputs": [],
   "source": [
    "stmp = timestamp()\n",
    "stmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b1b33",
   "metadata": {},
   "source": [
    "### 3. Define the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device is:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5225a",
   "metadata": {},
   "source": [
    "### 4. Custom functions for `marginal entropy`, `conditional entropy`, and `KL divergence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9286146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginal entropy\n",
    "def get_entropy_1D(xxx):\n",
    "    return (-torch.sum(xxx * torch.log(xxx + 1e-8)))\n",
    "\n",
    "# conditional entropy\n",
    "def get_entropy_2D(xxx):\n",
    "    return (-torch.sum(xxx * torch.log(xxx + 1e-8), dim=1))\n",
    "\n",
    "# KL divergence\n",
    "def get_KLD_1D(ppp, qqq, batch_mean=True):\n",
    "    tmp = torch.sum((ppp * torch.log(ppp + 1e-8) - ppp * torch.log(qqq + 1e-8)), dim=1)\n",
    "    if batch_mean:\n",
    "        return torch.mean(tmp)\n",
    "    else:\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8402d74",
   "metadata": {},
   "source": [
    "### 5. Set different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 765\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbf5677",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c0557",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_batch_size = 16\n",
    "rgb_input = 3\n",
    "z_channels = 16\n",
    "n_clusters = 14 # change it to 10, 11, 12, 13, 15, or 16 for other partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bdf987",
   "metadata": {},
   "source": [
    "### 6. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_src = '/project/dsc-is/nono/Documents/kpc/dat0'\n",
    "data_src = 'slice128_Block2_11K.npy'\n",
    "\n",
    "print(os.path.join(dir_src, data_src))\n",
    "\n",
    "kpc_dataset = np.load(os.path.join(dir_src, data_src))\n",
    "kpc_dataset = kpc_dataset[:, 0, :, :, :]\n",
    "\n",
    "print(kpc_dataset.shape)\n",
    "N_SAMPLE, HEIGHT, WIDTH, CHANNELS = kpc_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0118e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_range = np.arange(N_SAMPLE)\n",
    "split = np.array_split(index_range, 11)\n",
    "test_dataset = split[10]\n",
    "training_dataset = np.setdiff1d(index_range, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of the training dataset:', len(training_dataset))\n",
    "print('Length of the test dataset:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30baf9",
   "metadata": {},
   "source": [
    "### 7. Custom functions for model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40aecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class history():\n",
    "    def __init__(self, keys):\n",
    "        self.values = {}\n",
    "        for k in keys:\n",
    "            self.values[k] = []\n",
    "        self.keys = keys\n",
    "        \n",
    "    def append(self, dict_hist):\n",
    "        for k in dict_hist.keys():\n",
    "            self.values[k].append(dict_hist[k])\n",
    "    \n",
    "    def mean(self, keys=None):\n",
    "        if (keys is None):\n",
    "            keys = self.keys\n",
    "        m = {}\n",
    "        for k in keys:\n",
    "            m[k] = np.round(np.mean(self.values[k]), 4)\n",
    "        return m\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return (self.values[key])\n",
    "    \n",
    "    def __str__(self):\n",
    "        get = self.mean(self.keys)\n",
    "        return ('\\t'.join([k + ': ' + str(get[k]) for k in self.keys]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83970efe",
   "metadata": {},
   "source": [
    "### 8. Custom functions for extracting batches of samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_list(idx, n_batch=10, batch_size=None, shuffle=True):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    if batch_size is not None:\n",
    "        n_batch = len(idx) // batch_size\n",
    "    batch_list = np.array_split(idx, n_batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa1948",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "def generate_batch(idx, kpc_dataset):\n",
    "    tmp = []\n",
    "    for i in idx:\n",
    "        xxx = transform(kpc_dataset[i])\n",
    "        tmp.append(xxx)\n",
    "    xxx_batch = torch.stack(tmp, dim=0)\n",
    "    return xxx_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e7874",
   "metadata": {},
   "source": [
    "### 9. Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99781eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.mean(kpc_dataset, axis=(0, 1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b0ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix, iy, nc = 128, 128, 3 # height, width, channels\n",
    "\n",
    "add_random_affine = transforms.RandomAffine(degrees=5, translate=(0.05, 0.05), scale=(0.95, 1.05), fill=(130, 97, 154))\n",
    "\n",
    "def generate_random_affine_batch(take_batch, data_src):\n",
    "    \n",
    "    tmp = np.empty((len(take_batch), ix, iy, nc))\n",
    "    \n",
    "    for a, i in enumerate(take_batch):\n",
    "        img_tmp = Image.fromarray(data_src[i])\n",
    "        img_tmp = add_random_affine(img_tmp)\n",
    "        tmp[a] = img_tmp\n",
    "    xxx = torch.tensor(tmp/255.0, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    return xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daebcf8",
   "metadata": {},
   "source": [
    "### 10. Set up directory for saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db743438",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'models_14'\n",
    "\n",
    "if not os.path.exists(task_name):\n",
    "    os.mkdir(task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6940c17c",
   "metadata": {},
   "source": [
    "### 11. Instantiate `linear` scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e434724",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseScheduler(num_timesteps=num_timesteps, beta_start=beta_start, beta_end=beta_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ec260",
   "metadata": {},
   "source": [
    "### 12. Neural network for deep learning-based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('conv1', nn.Conv2d(in_channels=z_channels, out_channels=128, kernel_size=4, stride=2,\n",
    "                                                      padding=1))\n",
    "        self.classifier.add_module('bnor1', nn.BatchNorm2d(num_features=128, affine=True, track_running_stats=True))\n",
    "        self.classifier.add_module('lrel1', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        self.classifier.add_module('conv2', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1))\n",
    "        self.classifier.add_module('bnor2', nn.BatchNorm2d(num_features=128, affine=True, track_running_stats=True))\n",
    "        self.classifier.add_module('lrel2', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        self.classifier.add_module('conv3', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1))\n",
    "        self.classifier.add_module('lrel3', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        self.classifier.add_module('conv4', nn.Conv2d(in_channels=128, out_channels=n_clusters, kernel_size=4, stride=1,\n",
    "                                                      padding=0))\n",
    "        self.classifier.add_module('lrel4', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        \n",
    "    def forward(self, lat):\n",
    "        out = self.classifier(lat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6acbcc",
   "metadata": {},
   "source": [
    "### 13. Visualize `Classifier` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d006378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(Classifier(), input_size=(16, 16, 32, 32)) # batch_size, z_channels, latent_height, latent_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f0a1a",
   "metadata": {},
   "source": [
    "### 14. Visualize `VQVAE` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cad8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(VQVAE(im_channels=rgb_input), input_size=(16, 3, 128, 128)) # batch_size, channels, height, width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5096f0",
   "metadata": {},
   "source": [
    "### 15. Visualize `UNet` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec6ebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(UNet(im_channels=z_channels, cls=n_clusters), input_size=[(16, 16, 32, 32), (16,)])\n",
    "# (batch_size, z_channels, latent_height, latent_width), (batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423588ee",
   "metadata": {},
   "source": [
    "### 16. Instantiate `UNet`, `VQVAE`, and `Classifier` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf34a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(im_channels=z_channels, cls=n_clusters).to(device)\n",
    "model.train()\n",
    "\n",
    "vq_vae = VQVAE(im_channels=rgb_input).to(device)\n",
    "vq_vae.eval()\n",
    "print('Loaded vq_vae checkpoint')\n",
    "vq_vae.load_state_dict(torch.load(os.path.join('../kpc_ldm', 'vqvae_autoencoder_ckpt.pth'), map_location=device,\n",
    "                                  weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cl = Classifier().to(device)\n",
    "model_cl.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ac8a44",
   "metadata": {},
   "source": [
    "### 17. Prepare to train the `cLDM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_loss = ['Loss', 'MSE', 'ME', 'CE', 'AF']\n",
    "loss_hist = history(['Epoch'] + key_loss)\n",
    "\n",
    "# setting up additional hyperparameters\n",
    "num_epochs = 600\n",
    "learning_rate = 0.003\n",
    "optimizer = optim.Adadelta(list(model.parameters()) + list(model_cl.parameters()), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "l_me = 0.1\n",
    "l_ce = 0.06\n",
    "l_af = 0.04"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c8604",
   "metadata": {},
   "source": [
    "### 18. Train the `cLDM `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e5d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "for param in vq_vae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    batch_list = make_batch_list(training_dataset, batch_size=select_batch_size)\n",
    "    \n",
    "    loss_tt = history(key_loss)\n",
    "    \n",
    "    for idx_tmp in batch_list:\n",
    "        optimizer.zero_grad()\n",
    "        xxx_tmp = generate_batch(idx_tmp, kpc_dataset)\n",
    "        im = xxx_tmp.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            im, _ = vq_vae.encode(im)\n",
    "    \n",
    "        out_cl = model_cl(im)\n",
    "        ppp_tmp = F.softmax(out_cl.reshape((-1, n_clusters)), dim=1)\n",
    "        ppp_mean = torch.mean(ppp_tmp, dim=0, keepdim=True)\n",
    "        entropy_marginal = get_entropy_1D(ppp_mean)\n",
    "        entropy_cond = torch.mean(get_entropy_2D(ppp_tmp))\n",
    "        cond_input = torch.argmax(out_cl.reshape((-1, n_clusters)), dim=1)\n",
    "        \n",
    "        xxa_tmp = generate_random_affine_batch(idx_tmp, kpc_dataset)\n",
    "        im_af = xxa_tmp.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            im_af, _ = vq_vae.encode(im_af)\n",
    "        \n",
    "        out_cl_af = model_cl(im_af)\n",
    "        ppa_tmp = F.softmax(out_cl_af.reshape((-1, n_clusters)), dim=1)\n",
    "        \n",
    "        loss_affine = get_KLD_1D(ppp_tmp, ppa_tmp)\n",
    "\n",
    "        noise = torch.randn_like(im).to(device)\n",
    "\n",
    "        t = torch.randint(low=0, high=num_timesteps, size=(im.shape[0],)).to(device)\n",
    "\n",
    "        noisy_im = scheduler.add_noise(im, noise, t)\n",
    "        noise_pred = model(noisy_im, t, cond_input=cond_input)\n",
    "\n",
    "        mse_loss = criterion(noise_pred, noise)\n",
    "        loss_tmp = mse_loss - l_me * entropy_marginal + l_ce * entropy_cond + l_af * loss_affine\n",
    "        \n",
    "        loss_tmp.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_tt.append({'Loss': loss_tmp.item(), 'MSE': mse_loss.item(), 'ME': entropy_marginal.item(),\n",
    "                        'CE': entropy_cond.item(), 'AF': loss_affine.item()})\n",
    "    \n",
    "    loss_hist.append({'Epoch': epoch_idx + 1})\n",
    "    loss_hist.append(loss_tt.mean())\n",
    "    \n",
    "    print('Epoch:', epoch_idx + 1, '\\t', str(loss_tt))\n",
    "    \n",
    "print('Done training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a5addc",
   "metadata": {},
   "source": [
    "### 19. Save models after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(task_name, f'unet_training_ckpt_{stmp}_{num_epochs}_{n_clusters}.pth'))\n",
    "torch.save(model_cl.state_dict(), os.path.join(task_name, f'classifier_training_ckpt_{stmp}_{num_epochs}_{n_clusters}.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
