{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edc56e6",
   "metadata": {},
   "source": [
    "# <center>This `.ipynb` file contains the code for the first finetuning of the `cLDM` architecture</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b88800",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65377e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from pfiles.unet_cond_base import UNet\n",
    "from pfiles.vqvae import VQVAE\n",
    "from pfiles.linear_noise_scheduler import LinearNoiseScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f37a3e",
   "metadata": {},
   "source": [
    "### 2. Define a stamp to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    time_cur = datetime.datetime.now()\n",
    "    stamp = time_cur.strftime('%Y%m%d')\n",
    "    return stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948fe6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stmp = timestamp()\n",
    "stmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140e8b4",
   "metadata": {},
   "source": [
    "### 3. Define the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device is:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b5e42d",
   "metadata": {},
   "source": [
    "### 4. Set different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 765\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 1000\n",
    "beta_start = 0.0001\n",
    "beta_end = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d917eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_batch_size = 16\n",
    "rgb_input = 3\n",
    "z_channels = 16\n",
    "n_clusters = 14 # change it to 10, 11, 12, 13, 15, or 16 for other partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e8fa2",
   "metadata": {},
   "source": [
    "### 5. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f287729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_src = '/project/dsc-is/nono/Documents/kpc/dat0'\n",
    "data_src = 'slice128_Block2_11K.npy'\n",
    "\n",
    "print(os.path.join(dir_src, data_src))\n",
    "\n",
    "kpc_dataset = np.load(os.path.join(dir_src, data_src))\n",
    "kpc_dataset = kpc_dataset[:, 0, :, :, :]\n",
    "\n",
    "print(kpc_dataset.shape)\n",
    "N_SAMPLE, HEIGHT, WIDTH, CHANNELS = kpc_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0118e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_range = np.arange(N_SAMPLE)\n",
    "split = np.array_split(index_range, 11)\n",
    "test_dataset = split[10]\n",
    "training_dataset = np.setdiff1d(index_range, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960ab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Length of the training dataset:', len(training_dataset))\n",
    "print('Length of the test dataset:', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701365f",
   "metadata": {},
   "source": [
    "### 6. Custom functions for `marginal entropy`, `conditional entropy`, and `KL divergence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ff79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class history():\n",
    "    def __init__(self, keys):\n",
    "        self.values = {}\n",
    "        for k in keys:\n",
    "            self.values[k] = []\n",
    "        self.keys = keys\n",
    "        \n",
    "    def append(self, dict_hist):\n",
    "        for k in dict_hist.keys():\n",
    "            self.values[k].append(dict_hist[k])\n",
    "    \n",
    "    def mean(self, keys=None):\n",
    "        if (keys is None):\n",
    "            keys = self.keys\n",
    "        m = {}\n",
    "        for k in keys:\n",
    "            m[k] = np.round(np.mean(self.values[k]), 4)\n",
    "        return m\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return (self.values[key])\n",
    "    \n",
    "    def __str__(self):\n",
    "        get = self.mean(self.keys)\n",
    "        return ('\\t'.join([k + ': ' + str(get[k]) for k in self.keys]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bf179",
   "metadata": {},
   "source": [
    "### 7. Custom functions for extracting batches of samples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_list(idx, n_batch=10, batch_size=None, shuffle=True):\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx)\n",
    "    if batch_size is not None:\n",
    "        n_batch = len(idx) // batch_size\n",
    "    batch_list = np.array_split(idx, n_batch)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa1948",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "def generate_batch(idx, kpc_dataset):\n",
    "    tmp = []\n",
    "    for i in idx:\n",
    "        xxx = transform(kpc_dataset[i])\n",
    "        tmp.append(xxx)\n",
    "    xxx_batch = torch.stack(tmp, dim=0)\n",
    "    return xxx_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf5c93",
   "metadata": {},
   "source": [
    "### 8. Set up directory for saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db743438",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'models_14'\n",
    "\n",
    "if not os.path.exists(task_name):\n",
    "    os.mkdir(task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137216c",
   "metadata": {},
   "source": [
    "### 9. Instantiate `linear` scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e434724",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LinearNoiseScheduler(num_timesteps=num_timesteps, beta_start=beta_start, beta_end=beta_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e98b3",
   "metadata": {},
   "source": [
    "### 10. Neural network for deep learning-based clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('conv1', nn.Conv2d(in_channels=z_channels, out_channels=128, kernel_size=4, stride=2,\n",
    "                                                      padding=1))\n",
    "        self.classifier.add_module('bnor1', nn.BatchNorm2d(num_features=128, affine=True, track_running_stats=True))\n",
    "        self.classifier.add_module('lrel1', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        self.classifier.add_module('conv2', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1))\n",
    "        self.classifier.add_module('bnor2', nn.BatchNorm2d(num_features=128, affine=True, track_running_stats=True))\n",
    "        self.classifier.add_module('lrel2', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        self.classifier.add_module('conv3', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=4, stride=2, padding=1))\n",
    "        self.classifier.add_module('lrel3', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        self.classifier.add_module('conv4', nn.Conv2d(in_channels=128, out_channels=n_clusters, kernel_size=4, stride=1,\n",
    "                                                      padding=0))\n",
    "        self.classifier.add_module('lrel4', nn.LeakyReLU(negative_slope=0.1, inplace=True))\n",
    "        \n",
    "    def forward(self, lat):\n",
    "        out = self.classifier(lat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f9963",
   "metadata": {},
   "source": [
    "### 11. Visualize `Classifier` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d006378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(Classifier(), input_size=(16, 16, 32, 32)) # batch_size, z_channels, latent_height, latent_width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27e0ae7",
   "metadata": {},
   "source": [
    "### 12. Visualize `VQVAE` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cad8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary(VQVAE(im_channels=rgb_input), input_size=(16, 3, 128, 128)) # batch_size, channels, height, width"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83057902",
   "metadata": {},
   "source": [
    "### 13. Visualize `UNet` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec6ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(UNet(im_channels=z_channels, cls=n_clusters), input_size=[(16, 16, 32, 32), (16,)])\n",
    "# (batch_size, z_channels, latent_height, latent_width), (batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc767cab",
   "metadata": {},
   "source": [
    "### 14. Instantiate `UNet`, `VQVAE`, and `Classifier` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf34a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(im_channels=z_channels, cls=n_clusters).to(device)\n",
    "model.eval()\n",
    "print('Loaded unet training checkpoint')\n",
    "model.load_state_dict(torch.load(os.path.join(task_name, 'unet_training_ckpt_20250127_600_14.pth'), map_location=device,\n",
    "                                 weights_only=True))\n",
    "\n",
    "vq_vae = VQVAE(im_channels=rgb_input).to(device)\n",
    "vq_vae.eval()\n",
    "print('Loaded vq_vae checkpoint')\n",
    "vq_vae.load_state_dict(torch.load(os.path.join('../kpc_ldm', 'vqvae_autoencoder_ckpt.pth'), map_location=device,\n",
    "                                  weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01a835",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cl = Classifier().to(device)\n",
    "model_cl.eval()\n",
    "print('Loaded model_cl training checkpoint')\n",
    "model_cl.load_state_dict(torch.load(os.path.join(task_name, 'classifier_training_ckpt_20250127_600_14.pth'),\n",
    "                                    map_location=device, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fb3e8",
   "metadata": {},
   "source": [
    "### 15. Prepare to train the `cLDM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a0f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_loss = ['MSE']\n",
    "loss_hist = history(['Epoch'] + key_loss)\n",
    "\n",
    "# setting up additional hyperparameters\n",
    "num_epochs = 130\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c86c59",
   "metadata": {},
   "source": [
    "### 16. Train the `cLDM `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e5d37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "for param in vq_vae.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch_idx in range(num_epochs):\n",
    "    batch_list = make_batch_list(training_dataset, batch_size=select_batch_size)\n",
    "    \n",
    "    loss_tt = history(key_loss)\n",
    "    \n",
    "    for idx_tmp in batch_list:\n",
    "        optimizer.zero_grad()\n",
    "        xxx_tmp = generate_batch(idx_tmp, kpc_dataset)\n",
    "        im = xxx_tmp.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            im, _ = vq_vae.encode(im)\n",
    "    \n",
    "        out_cl = model_cl(im)\n",
    "        cond_input = torch.argmax(out_cl.reshape((-1, n_clusters)), dim=1)\n",
    "\n",
    "        noise = torch.randn_like(im).to(device)\n",
    "\n",
    "        t = torch.randint(low=0, high=num_timesteps, size=(im.shape[0],)).to(device)\n",
    "\n",
    "        noisy_im = scheduler.add_noise(im, noise, t)\n",
    "        noise_pred = model(noisy_im, t, cond_input=cond_input)\n",
    "\n",
    "        mse_loss = criterion(noise_pred, noise)\n",
    "        \n",
    "        mse_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_tt.append({'MSE': mse_loss.item()})\n",
    "    \n",
    "    loss_hist.append({'Epoch': epoch_idx + 1})\n",
    "    loss_hist.append(loss_tt.mean())\n",
    "    \n",
    "    print('Epoch:', epoch_idx + 1, '\\t', str(loss_tt))\n",
    "\n",
    "print('Done training...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d128cfc0",
   "metadata": {},
   "source": [
    "### 17. Save model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb820713",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(task_name, f'unet_finetuning1_ckpt_{stmp}_{num_epochs}_{n_clusters}.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
